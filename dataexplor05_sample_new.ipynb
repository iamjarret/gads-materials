{
 "metadata": {
  "name": "",
  "signature": "sha256:c3821a8e26a22fef08aa8077a6765b695a7f00696ef8ad565fbe37d9c6fcf6eb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image\n",
      "Image(url=\"http://www.contemporaryartdaily.com/wp-content/uploads/2011/07/14-FromPoint3Panels.jpg\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<img src=\"http://www.contemporaryartdaily.com/wp-content/uploads/2011/07/14-FromPoint3Panels.jpg\"/>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 338,
       "text": [
        "<IPython.core.display.Image at 0x97054d0>"
       ]
      }
     ],
     "prompt_number": 338
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 339
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv('https://raw.githubusercontent.com/TeachingDataScience/data-science-course/forstudentviewing/12_Naive_Bayes/twitter_training/sts_gold_tweet.csv',';')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 340
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 340
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "import numpy as np\n",
      "from itertools import product\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "models = [MultinomialNB(), BernoulliNB(), LogisticRegression()]\n",
      "num_words = [int(x) for x in list(np.linspace(20,1000,20))]\n",
      "vectorizers = [CountVectorizer(ngram_range=(1,1), stop_words='english', max_features=words) for words in num_words]\n",
      "\n",
      "results ={}\n",
      "for model, vectorizer in product(models, vectorizers):\n",
      "    modelname = \"%s%d\"% (model.__class__.__name__, vectorizer.max_features)\n",
      "    kf = KFold(df.shape[0], 5)\n",
      "    cv_accuracies = []\n",
      "    for train, test in kf:\n",
      "        training_bag = vectorizer.fit_transform(df['tweet'][train])\n",
      "        testing_bag = vectorizer.transform(df['tweet'][test])\n",
      "        model.fit(training_bag, df['polarity'][train])\n",
      "        cv_accuracy = accuracy_score(model.predict(testing_bag), df['polarity'][test])\n",
      "        cv_accuracies.append(cv_accuracy)\n",
      "    results[modelname]={\n",
      "        'model': model.__class__.__name__,\n",
      "        'num_words': vectorizer.max_features,\n",
      "        'cv_accuracies': cv_accuracies,\n",
      "        'avg_accuracy': sum(cv_accuracies)/5\n",
      "    }\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 341
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame.from_dict(results.values()).sort(['avg_accuracy'], ascending=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>avg_accuracy</th>\n",
        "      <th>cv_accuracies</th>\n",
        "      <th>model</th>\n",
        "      <th>num_words</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>59</th>\n",
        "      <td> 0.817615</td>\n",
        "      <td> [0.791154791155, 0.746928746929, 0.83538083538...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28</th>\n",
        "      <td> 0.815647</td>\n",
        "      <td> [0.791154791155, 0.742014742015, 0.83292383292...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  638</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22</th>\n",
        "      <td> 0.815647</td>\n",
        "      <td> [0.786240786241, 0.749385749386, 0.82800982801...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  535</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>35</th>\n",
        "      <td> 0.814666</td>\n",
        "      <td> [0.786240786241, 0.749385749386, 0.83292383292...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  948</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td> 0.813681</td>\n",
        "      <td> [0.796068796069, 0.746928746929, 0.82800982801...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  690</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>52</th>\n",
        "      <td> 0.813192</td>\n",
        "      <td> [0.786240786241, 0.744471744472, 0.83046683046...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  896</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>38</th>\n",
        "      <td> 0.813192</td>\n",
        "      <td> [0.793611793612, 0.746928746929, 0.82800982801...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  845</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>49</th>\n",
        "      <td> 0.812206</td>\n",
        "      <td> [0.793611793612, 0.746928746929, 0.82063882063...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  742</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td> 0.812206</td>\n",
        "      <td> [0.796068796069, 0.744471744472, 0.82063882063...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  793</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>33</th>\n",
        "      <td> 0.811714</td>\n",
        "      <td> [0.776412776413, 0.746928746929, 0.82309582309...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  484</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>44</th>\n",
        "      <td> 0.810735</td>\n",
        "      <td> [0.783783783784, 0.742014742015, 0.82555282555...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td> 1000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>32</th>\n",
        "      <td> 0.808284</td>\n",
        "      <td> [0.769041769042, 0.714987714988, 0.83046683046...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td> 0.808282</td>\n",
        "      <td> [0.771498771499, 0.719901719902, 0.82309582309...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  535</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td> 0.805812</td>\n",
        "      <td> [0.771498771499, 0.737100737101, 0.82555282555...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  432</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>39</th>\n",
        "      <td> 0.805330</td>\n",
        "      <td> [0.764127764128, 0.719901719902, 0.82063882063...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  484</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>24</th>\n",
        "      <td> 0.804350</td>\n",
        "      <td> [0.759213759214, 0.712530712531, 0.82309582309...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td> 1000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td> 0.804350</td>\n",
        "      <td> [0.766584766585, 0.697788697789, 0.82800982801...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  638</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>31</th>\n",
        "      <td> 0.802872</td>\n",
        "      <td> [0.7542997543, 0.707616707617, 0.82800982801, ...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  845</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td> 0.802381</td>\n",
        "      <td> [0.756756756757, 0.710073710074, 0.82309582309...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  948</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td> 0.802370</td>\n",
        "      <td> [0.769041769042, 0.734643734644, 0.82063882063...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  381</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>51</th>\n",
        "      <td> 0.801893</td>\n",
        "      <td> [0.759213759214, 0.692874692875, 0.83046683046...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  690</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td> 0.801888</td>\n",
        "      <td> [0.7542997543, 0.719901719902, 0.820638820639,...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  432</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>37</th>\n",
        "      <td> 0.801888</td>\n",
        "      <td> [0.756756756757, 0.710073710074, 0.82309582309...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  896</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td> 0.801887</td>\n",
        "      <td> [0.761670761671, 0.707616707617, 0.82800982801...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  793</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>41</th>\n",
        "      <td> 0.801394</td>\n",
        "      <td> [0.761670761671, 0.727272727273, 0.81081081081...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  381</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td> 0.800420</td>\n",
        "      <td> [0.751842751843, 0.687960687961, 0.83046683046...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  793</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>54</th>\n",
        "      <td> 0.800410</td>\n",
        "      <td> [0.759213759214, 0.734643734644, 0.80835380835...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  277</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>42</th>\n",
        "      <td> 0.799929</td>\n",
        "      <td> [0.751842751843, 0.692874692875, 0.82800982801...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  742</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>30</th>\n",
        "      <td> 0.798440</td>\n",
        "      <td> [0.756756756757, 0.744471744472, 0.80589680589...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  329</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td> 0.797964</td>\n",
        "      <td> [0.756756756757, 0.685503685504, 0.82063882063...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  845</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>56</th>\n",
        "      <td> 0.797955</td>\n",
        "      <td> [0.751842751843, 0.714987714988, 0.82309582309...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  638</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23</th>\n",
        "      <td> 0.797464</td>\n",
        "      <td> [0.751842751843, 0.702702702703, 0.82555282555...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  742</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td> 0.796973</td>\n",
        "      <td> [0.744471744472, 0.710073710074, 0.82800982801...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  535</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>53</th>\n",
        "      <td> 0.796483</td>\n",
        "      <td> [0.751842751843, 0.72972972973, 0.798525798526...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  329</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>48</th>\n",
        "      <td> 0.796480</td>\n",
        "      <td> [0.761670761671, 0.727272727273, 0.79852579852...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  277</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50</th>\n",
        "      <td> 0.795991</td>\n",
        "      <td> [0.751842751843, 0.707616707617, 0.81572481572...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  690</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>58</th>\n",
        "      <td> 0.795990</td>\n",
        "      <td> [0.751842751843, 0.707616707617, 0.82555282555...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>46</th>\n",
        "      <td> 0.795510</td>\n",
        "      <td> [0.734643734644, 0.687960687961, 0.81818181818...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td> 1000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td> 0.795005</td>\n",
        "      <td> [0.751842751843, 0.727272727273, 0.80098280098...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  226</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td> 0.794514</td>\n",
        "      <td> [0.751842751843, 0.70515970516, 0.813267813268...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  381</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td> 0.794511</td>\n",
        "      <td> [0.7542997543, 0.710073710074, 0.825552825553,...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  277</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>47</th>\n",
        "      <td> 0.794022</td>\n",
        "      <td> [0.756756756757, 0.70515970516, 0.813267813268...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  329</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>55</th>\n",
        "      <td> 0.793543</td>\n",
        "      <td> [0.734643734644, 0.678132678133, 0.81818181818...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  948</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26</th>\n",
        "      <td> 0.793542</td>\n",
        "      <td> [0.737100737101, 0.68058968059, 0.818181818182...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  896</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>34</th>\n",
        "      <td> 0.792056</td>\n",
        "      <td> [0.732186732187, 0.717444717445, 0.81326781326...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  484</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27</th>\n",
        "      <td> 0.792053</td>\n",
        "      <td> [0.746928746929, 0.712530712531, 0.81572481572...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  432</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>40</th>\n",
        "      <td> 0.790088</td>\n",
        "      <td> [0.749385749386, 0.697788697789, 0.82309582309...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  226</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td> 0.789602</td>\n",
        "      <td> [0.749385749386, 0.710073710074, 0.79606879606...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  226</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td> 0.781735</td>\n",
        "      <td> [0.734643734644, 0.697788697789, 0.79606879606...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  174</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td> 0.779275</td>\n",
        "      <td> [0.722358722359, 0.683046683047, 0.80835380835...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  174</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>45</th>\n",
        "      <td> 0.778295</td>\n",
        "      <td> [0.737100737101, 0.695331695332, 0.79115479115...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  174</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td> 0.777313</td>\n",
        "      <td> [0.737100737101, 0.68058968059, 0.798525798526...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>  123</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>36</th>\n",
        "      <td> 0.776327</td>\n",
        "      <td> [0.724815724816, 0.685503685504, 0.80098280098...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>  123</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>43</th>\n",
        "      <td> 0.770921</td>\n",
        "      <td> [0.714987714988, 0.646191646192, 0.82309582309...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>  123</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25</th>\n",
        "      <td> 0.748321</td>\n",
        "      <td> [0.673218673219, 0.628992628993, 0.79361179361...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>   71</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td> 0.746845</td>\n",
        "      <td> [0.665847665848, 0.621621621622, 0.79361179361...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>   71</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29</th>\n",
        "      <td> 0.744391</td>\n",
        "      <td> [0.673218673219, 0.597051597052, 0.79852579852...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>   71</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>57</th>\n",
        "      <td> 0.707087</td>\n",
        "      <td> [0.599508599509, 0.484029484029, 0.75921375921...</td>\n",
        "      <td>      MultinomialNB</td>\n",
        "      <td>   20</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td> 0.703643</td>\n",
        "      <td> [0.592137592138, 0.488943488943, 0.75675675675...</td>\n",
        "      <td>        BernoulliNB</td>\n",
        "      <td>   20</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21</th>\n",
        "      <td> 0.701678</td>\n",
        "      <td> [0.609336609337, 0.464373464373, 0.74938574938...</td>\n",
        "      <td> LogisticRegression</td>\n",
        "      <td>   20</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 342,
       "text": [
        "    avg_accuracy                                      cv_accuracies  \\\n",
        "59      0.817615  [0.791154791155, 0.746928746929, 0.83538083538...   \n",
        "28      0.815647  [0.791154791155, 0.742014742015, 0.83292383292...   \n",
        "22      0.815647  [0.786240786241, 0.749385749386, 0.82800982801...   \n",
        "35      0.814666  [0.786240786241, 0.749385749386, 0.83292383292...   \n",
        "15      0.813681  [0.796068796069, 0.746928746929, 0.82800982801...   \n",
        "52      0.813192  [0.786240786241, 0.744471744472, 0.83046683046...   \n",
        "38      0.813192  [0.793611793612, 0.746928746929, 0.82800982801...   \n",
        "49      0.812206  [0.793611793612, 0.746928746929, 0.82063882063...   \n",
        "4       0.812206  [0.796068796069, 0.744471744472, 0.82063882063...   \n",
        "33      0.811714  [0.776412776413, 0.746928746929, 0.82309582309...   \n",
        "44      0.810735  [0.783783783784, 0.742014742015, 0.82555282555...   \n",
        "32      0.808284  [0.769041769042, 0.714987714988, 0.83046683046...   \n",
        "20      0.808282  [0.771498771499, 0.719901719902, 0.82309582309...   \n",
        "17      0.805812  [0.771498771499, 0.737100737101, 0.82555282555...   \n",
        "39      0.805330  [0.764127764128, 0.719901719902, 0.82063882063...   \n",
        "24      0.804350  [0.759213759214, 0.712530712531, 0.82309582309...   \n",
        "13      0.804350  [0.766584766585, 0.697788697789, 0.82800982801...   \n",
        "31      0.802872  [0.7542997543, 0.707616707617, 0.82800982801, ...   \n",
        "11      0.802381  [0.756756756757, 0.710073710074, 0.82309582309...   \n",
        "5       0.802370  [0.769041769042, 0.734643734644, 0.82063882063...   \n",
        "51      0.801893  [0.759213759214, 0.692874692875, 0.83046683046...   \n",
        "10      0.801888  [0.7542997543, 0.719901719902, 0.820638820639,...   \n",
        "37      0.801888  [0.756756756757, 0.710073710074, 0.82309582309...   \n",
        "9       0.801887  [0.761670761671, 0.707616707617, 0.82800982801...   \n",
        "41      0.801394  [0.761670761671, 0.727272727273, 0.81081081081...   \n",
        "16      0.800420  [0.751842751843, 0.687960687961, 0.83046683046...   \n",
        "54      0.800410  [0.759213759214, 0.734643734644, 0.80835380835...   \n",
        "42      0.799929  [0.751842751843, 0.692874692875, 0.82800982801...   \n",
        "30      0.798440  [0.756756756757, 0.744471744472, 0.80589680589...   \n",
        "2       0.797964  [0.756756756757, 0.685503685504, 0.82063882063...   \n",
        "56      0.797955  [0.751842751843, 0.714987714988, 0.82309582309...   \n",
        "23      0.797464  [0.751842751843, 0.702702702703, 0.82555282555...   \n",
        "18      0.796973  [0.744471744472, 0.710073710074, 0.82800982801...   \n",
        "53      0.796483  [0.751842751843, 0.72972972973, 0.798525798526...   \n",
        "48      0.796480  [0.761670761671, 0.727272727273, 0.79852579852...   \n",
        "50      0.795991  [0.751842751843, 0.707616707617, 0.81572481572...   \n",
        "58      0.795990  [0.751842751843, 0.707616707617, 0.82555282555...   \n",
        "46      0.795510  [0.734643734644, 0.687960687961, 0.81818181818...   \n",
        "0       0.795005  [0.751842751843, 0.727272727273, 0.80098280098...   \n",
        "3       0.794514  [0.751842751843, 0.70515970516, 0.813267813268...   \n",
        "14      0.794511  [0.7542997543, 0.710073710074, 0.825552825553,...   \n",
        "47      0.794022  [0.756756756757, 0.70515970516, 0.813267813268...   \n",
        "55      0.793543  [0.734643734644, 0.678132678133, 0.81818181818...   \n",
        "26      0.793542  [0.737100737101, 0.68058968059, 0.818181818182...   \n",
        "34      0.792056  [0.732186732187, 0.717444717445, 0.81326781326...   \n",
        "27      0.792053  [0.746928746929, 0.712530712531, 0.81572481572...   \n",
        "40      0.790088  [0.749385749386, 0.697788697789, 0.82309582309...   \n",
        "12      0.789602  [0.749385749386, 0.710073710074, 0.79606879606...   \n",
        "1       0.781735  [0.734643734644, 0.697788697789, 0.79606879606...   \n",
        "7       0.779275  [0.722358722359, 0.683046683047, 0.80835380835...   \n",
        "45      0.778295  [0.737100737101, 0.695331695332, 0.79115479115...   \n",
        "19      0.777313  [0.737100737101, 0.68058968059, 0.798525798526...   \n",
        "36      0.776327  [0.724815724816, 0.685503685504, 0.80098280098...   \n",
        "43      0.770921  [0.714987714988, 0.646191646192, 0.82309582309...   \n",
        "25      0.748321  [0.673218673219, 0.628992628993, 0.79361179361...   \n",
        "6       0.746845  [0.665847665848, 0.621621621622, 0.79361179361...   \n",
        "29      0.744391  [0.673218673219, 0.597051597052, 0.79852579852...   \n",
        "57      0.707087  [0.599508599509, 0.484029484029, 0.75921375921...   \n",
        "8       0.703643  [0.592137592138, 0.488943488943, 0.75675675675...   \n",
        "21      0.701678  [0.609336609337, 0.464373464373, 0.74938574938...   \n",
        "\n",
        "                 model  num_words  \n",
        "59       MultinomialNB        587  \n",
        "28       MultinomialNB        638  \n",
        "22       MultinomialNB        535  \n",
        "35       MultinomialNB        948  \n",
        "15       MultinomialNB        690  \n",
        "52       MultinomialNB        896  \n",
        "38       MultinomialNB        845  \n",
        "49       MultinomialNB        742  \n",
        "4        MultinomialNB        793  \n",
        "33       MultinomialNB        484  \n",
        "44       MultinomialNB       1000  \n",
        "32         BernoulliNB        587  \n",
        "20         BernoulliNB        535  \n",
        "17       MultinomialNB        432  \n",
        "39         BernoulliNB        484  \n",
        "24  LogisticRegression       1000  \n",
        "13         BernoulliNB        638  \n",
        "31  LogisticRegression        845  \n",
        "11  LogisticRegression        948  \n",
        "5        MultinomialNB        381  \n",
        "51         BernoulliNB        690  \n",
        "10         BernoulliNB        432  \n",
        "37  LogisticRegression        896  \n",
        "9   LogisticRegression        793  \n",
        "41         BernoulliNB        381  \n",
        "16         BernoulliNB        793  \n",
        "54       MultinomialNB        277  \n",
        "42         BernoulliNB        742  \n",
        "30       MultinomialNB        329  \n",
        "2          BernoulliNB        845  \n",
        "56  LogisticRegression        638  \n",
        "23  LogisticRegression        742  \n",
        "18  LogisticRegression        535  \n",
        "53         BernoulliNB        329  \n",
        "48         BernoulliNB        277  \n",
        "50  LogisticRegression        690  \n",
        "58  LogisticRegression        587  \n",
        "46         BernoulliNB       1000  \n",
        "0        MultinomialNB        226  \n",
        "3   LogisticRegression        381  \n",
        "14  LogisticRegression        277  \n",
        "47  LogisticRegression        329  \n",
        "55         BernoulliNB        948  \n",
        "26         BernoulliNB        896  \n",
        "34  LogisticRegression        484  \n",
        "27  LogisticRegression        432  \n",
        "40  LogisticRegression        226  \n",
        "12         BernoulliNB        226  \n",
        "1        MultinomialNB        174  \n",
        "7   LogisticRegression        174  \n",
        "45         BernoulliNB        174  \n",
        "19         BernoulliNB        123  \n",
        "36       MultinomialNB        123  \n",
        "43  LogisticRegression        123  \n",
        "25         BernoulliNB         71  \n",
        "6        MultinomialNB         71  \n",
        "29  LogisticRegression         71  \n",
        "57       MultinomialNB         20  \n",
        "8          BernoulliNB         20  \n",
        "21  LogisticRegression         20  "
       ]
      }
     ],
     "prompt_number": 342
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['percent_capitalized'] = df.tweet.apply(lambda x: sum([float(x[i] == x.upper()[i]) for i in range(len(x))])/len(x))\n",
      "df['exclamation'] = df.tweet.apply(lambda x: 1 if \"!\" in x else 0)\n",
      "df['at'] = df.tweet.apply(lambda x: 1 if \"@\" in x else 0)\n",
      "df['link'] = df.tweet.apply(lambda x: 1 if \"http\" in x else 0)\n",
      "\n",
      "vectorizer = CountVectorizer(ngram_range=(1,1), stop_words='english', max_features=587)\n",
      "bag_of_words = vectorizer.fit_transform(df['tweet'])\n",
      "model1=MultinomialNB()\n",
      "model1.fit(bag_of_words, df.polarity)\n",
      "predict = model1.predict(bag_of_words)\n",
      "df['polarity_hat']=np.array(predict)\n",
      "modelcovar = ['polarity_hat','percent_capitalized','exclamation','at','link']\n",
      "model2 = LogisticRegression()\n",
      "model2.fit(df[modelcovar],df.polarity)\n",
      "predictions = model2.predict(df[modelcovar])\n",
      "print accuracy_score(predictions, df['polarity'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.891347099312\n"
       ]
      }
     ],
     "prompt_number": 361
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "class ModelTester:\n",
      "    def __init__(self):\n",
      "        self.models = []\n",
      "        \n",
      "    def add_model(self, name, modeltype, pre_processing = \"x\", vectorizer_args = {}, model_args={}):\n",
      "        vectorizer = CountVectorizer(**vectorizer_args)\n",
      "        model = modeltype(**model_args)\n",
      "        post_processing = df.tweet.apply(lambda x: eval(pre_processing))\n",
      "        bag_of_words = vectorizer.fit_transform(post_processing)\n",
      "        model.fit(bag_of_words, df.polarity)\n",
      "        data = {\n",
      "            'name':name,\n",
      "            'vectorizer':vectorizer,\n",
      "            'pre_processing': pre_processing,\n",
      "            'model': model\n",
      "        }\n",
      "        self.models.append(data)\n",
      "        \n",
      "    def test_models(self, tweet):\n",
      "        for model in self.models:\n",
      "            process = lambda x: eval(model['pre_processing'])\n",
      "            processed_tweet = process(tweet)\n",
      "            bag_of_words = model['vectorizer'].transform([processed_tweet])\n",
      "            print \"%s: %s\" % (model['name'],model['model'].predict(bag_of_words))\n",
      "        \n",
      "        \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modeltester= ModelTester()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modeltester.add_model(\"Jarret\", MultinomialNB, vectorizer_args={'max_features':587})\n",
      "modeltester.add_model(\"In Class Demo\", LogisticRegression, vectorizer_args={'ngram_range':(1,2)})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modeltester.test_models(\"MODEL TESTER IS AWESOME!\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modeltester.test_models(\"UGHHHH IT DOESNT LIKE LASSO!\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}