<?xml version="1.0"?>
<presentation>
	<info company="Data Science" logo="ga-logo.png"/>
	
	<title title= "First Quarter Course Review" date="October 13th, 2014" people="Jarret Petrillo, jarret.petrillo@gmail.com"/>

	<section name="Using the Command Line w/ Git"/>

	<codeslide title="Helpful Commands">
		<codeblock>
			<l focus="1">cd ~/notebooks</l>
			<l>changes current directory to the notebook folder in vagrant</l>
			<l> </l>
			<l focus="1">cp ~/vagrant/file2movefromlocaldrive ~/notebooks</l>
			<l>Moves a file from the local drive into the virtual environ</l>
		</codeblock>

		<codeblock>
			<l>cd ~/notebooks/fall-2014-assignments</l>
			<l focus="1">git remote add origin https://github.com/gads14-nyc/fall_2014_assignments.git</l>
			<l>Bookmarks the typed git repo with the tag name "origin"</l>
			<l>Note: different folders can use the same remote name</l>
			<l> </l>
			<l focus="1">git pull origin master</l>
			<l>Copies any changes into your local directory from origin repo</l>
			<l> </l>
			<l focus="1">git add filetosubmit</l>
			<l focus="1">git commit -m "Added filetosubmit"</l>
			<l focus="1">git push origin master</l>
			<l>Uploads added file to online repo</l>
		</codeblock>
	</codeslide>

	<section name="Linear Regression"/>

	<slide title="Framework">
		<p>Simple linear regression uncovers basic correlations</p>
		<p>Least squares estimation is sensitive to overfitting and heteroskedasticity</p>
		<p>Overfitting is the inclusion of extra covariates</p>
		<p>Heteroskedasticity is the presence of correlation between error terms and means that the iid (independently identically distributed) assumption of linear regression doesn't hold</p>
		<p>Ridge regression is an alternative linear estimation algorithm that works even in the presence of mild heteroskedasticity</p>
		<p>Lasso regression is another estimation algorithm that is robust to overfitting</p>
	</slide>

	<codeslide title="Helpful Functions">
		<codeblock>
			<l focus="1">import statsmodels.api as sm</l>
			<l focus="1">model = sm.ols(y, X)</l>
			<l focus="1">results = model.fit()</l>
			<l focus="1">results.summary()</l>
		</codeblock>

		<codeblock>
			<l>from sklearn.linear_models import LinearRegression</l>
			<l>linear_model = LinearRegression(fit_intercept=True)</l>
			<l>linear_model.fit(X,y)</l>
			<l focus="1">y_hat = linear_model.predict(X)</l>
		</codeblock>
	</codeslide>

	<section name="Covariate Selection Using Cross Validation"/>

	<codeslide title="1-Fold CV: Pseudo Code">
		<codeblock>
			<l focus="1">1. Start with a list of potential models saved in a dictionary</l>
			<l indent="1">models = {'model01': ['Infrared02'], 'model02':['ELEV','Infrared02']}</l>
			<l focus="1">2. Divide data set into test and train subsets</l>
			<l focus="1">3. On the training subset fit each model</l>
			<l focus="1">4. Save the mean squared error for each model in a dictionary</l>
			<l focus="1">5. Sort the dictionary</l>
			<l indent="1">results = {'model01': 0.553, 'model02': 0.434}</l>
			<l focus="1">6. Choose the model with the lowest mean squared error</l>
		</codeblock>
	</codeslide>

	<codeslide title="K-Fold CV: Pseudo Code">
		<codeblock>
			<l focus="1">1. Start with a list of potential models saved in a dictionary</l>
			<l focus="1">2. for each k repeat steps 2, 3, and 4 above saving the results in a list in a dictionary</l>
			<l indent="1">results = {'model01': [0.533, 0.513, 0.567], 'model02': [0.475, 0.469, 0.458]}</l>
			<l focus="1">3. Convert list of mean squared errors into a single value by taking the average</l>
			<l indent="1">results = {'model01': 0.536, 'model02': 0.464}</l>
			<l focus="1">4. Sort the dictionary</l>
			<l focus="1">5. Choose the model with the lowest average mean squared error</l>
		</codeblock>
	</codeslide>

	<codeslide title="Helpful Functions">
		<codeblock>
			<l focus="1">from sklearn.cross_validation import KFold</l>
			<l>Returns a tuple (train, test) of 0/1 vectors</l>
			<l>data[train] returns training set</l>
			<l> </l>
			<l focus="1">from sklearn.metrics import mean_squared_error</l>
			<l>For two vector inputs returns the mean sqaured error</l>
		</codeblock>
		<codeblock>
			<l>results = {'model01': 0.536, 'model02': 0.464}</l>
			<l focus="1">sort(results, key=results.get, reverse=True)</l>
			<l>returns a sorted list from a dictionary</l>
		</codeblock>
	</codeslide>

</presentation>
